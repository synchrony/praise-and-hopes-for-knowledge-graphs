Tech has recently given us three big problems:
  * Loneliness
  * Fake news (including from bots)
  * The echo chamber effect: Wherein people with similar opinions reinforce each others' beliefs, easily diverging from the truth.

We might solve all three problems, by adding an open-ended reputation game to our social networks.


# Willing credulity, and the Turing test

In the classic Turing test, a human judge converses with a machine and a human, initially unaware of which is which. They cannot see each other, and they communicate through text. The purpose of the test is to determine whether the machine can fool someone into believing it is human.

A recent study estimates that [up to 15% of Twitter users are robots](https://www.cnbc.com/2017/03/10/nearly-48-million-twitter-accounts-could-be-bots-says-study.html); Twitter itself is willing to admit [as much as 8.5%](https://qz.com/248063/twitter-admits-that-as-many-as-23-million-of-its-active-users-are-actually-bots/). Lots of Twitter users have interacted with these robots, thinking they were real. [Bots may have changed the outcome of the US election](https://www.nytimes.com/2017/09/07/us/politics/russia-facebook-twitter-election.html) in 2016. How can that be, when [no algorithm has come anywhere close](http://isturingtestpassed.github.io/) to passing the Turing test?

The answer is simple: We don't tend to rigorously question the sources of news before we share it. When we agree with someone, we interrogate lightly, if at all. And when we disagree, we engage ineffectively.

I include myself in that. If I come across a headline on Facebook I disagree with, I'll post my doubts, ask pointed questions, and almost never seem to get satisfactory answers. If I agree with the headline, I might share it before even reading the article. I know that's bad, and I try to remember to be more critical, and I wish others would, too.

But the responsibility would not have to be ours alone. The social network could help us be more critical -- and without exercising its own preferences, or any kind of influence.


# An open-ended game

Dancing is an open-ended game. On the dance floor, there's no particular concrete shared goal. There's no triple-axel maneuver which, when accomplished, allows someone to know that they've "won". Every dancer is aiming for something different, and everyone judging the dancers is judging something different. 

Sharing information, too, is an open-ended game. (By "sharing" I mean giving, receiving or both.) Some people want to share practical information, like recipes or car advice. Some want to share jokes, or stories, or art. Some want to influence opinion; some want their opinions influenced.


# from here it's a mess

Information is powerful when it is allowed to stick. Like the "organic" label, or those warning labels that cigarrettes cause cancer, or wearing a purple heart. It's why bad people like nondisclosure agreements.

Reputation is open-ended. Someone can have a reputation for anything -- backflips, or chess, or burping, or standing still. The internet has not narrowed that menu. It has, however, brought undue focus to one particular variety of reputation: the number of people one has reached.

Page views, likes, retweets -- these are like scores in a video game. They are salient, always in view, and often treated as somethinig to maximize. More important kinds of information -- which questions were you able to answer? how many minds did you change? how many people did you help? -- are much harder to determine. They don't follow you around like a score in a game.

But they could!

Fake news spreads because X believes Y when Y doesn't know what they're talking about. It would be arrogant for a social network to label someone as such. But it seems perfectly fine for it to record the fact that *someone* *said* Y doesn't know what they're talking about. And higher-level metadata -- judge the judgements, judge the judges, etc.
